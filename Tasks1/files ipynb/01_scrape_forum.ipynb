{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVFKjCfkBDeF",
        "outputId": "413f2ebb-e2b1-4fd9-b907-61b61735e099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p /content/drive/MyDrive/agnos-rag/data/raw"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q playwright bs4 nest_asyncio tldextract\n",
        "!playwright install chromium -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceCRQIL8Haic",
        "outputId": "7834682d-fad5-4639-a660-515699ed2e83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error: unknown option '-q'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports & Config\n",
        "import os, re, asyncio, urllib.parse\n",
        "from datetime import datetime, timezone, timedelta\n",
        "from bs4 import BeautifulSoup\n",
        "from playwright.async_api import async_playwright\n",
        "import nest_asyncio, tldextract\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "BASE_URL    = \"https://www.agnoshealth.com/forums\"\n",
        "SAVE_DIR    = \"/content/drive/MyDrive/agnos-rag/data/raw\"\n",
        "MAX_THREADS = 10\n",
        "WAIT_UNTIL  = \"networkidle\"\n",
        "\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "def slugify(text, maxlen=60):\n",
        "    text = re.sub(r\"\\s+\", \"-\", text.strip())\n",
        "    text = re.sub(r\"[^A-Za-z0-9\\-]+\", \"\", text)\n",
        "    text = re.sub(r\"-{2,}\", \"-\", text).strip(\"-\")\n",
        "    return text[:maxlen] if text else \"untitled\"\n",
        "\n",
        "def abs_url(base, href):\n",
        "    return urllib.parse.urljoin(base, href) if href else None\n",
        "\n",
        "# เวลาไทย (Asia/Bangkok, +07:00)\n",
        "def now_iso_bkk():\n",
        "    return datetime.now(timezone(timedelta(hours=7))).isoformat()"
      ],
      "metadata": {
        "id": "UScKIbbqI7h7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def scrape_forum_html():\n",
        "    domain_ok = tldextract.extract(BASE_URL).registered_domain\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        page = await browser.new_page(user_agent=\"Mozilla/5.0 (Colab/Playwright)\")\n",
        "\n",
        "        await page.goto(BASE_URL, wait_until=WAIT_UNTIL)\n",
        "        home_html = await page.content()\n",
        "\n",
        "        ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "        home_path = os.path.join(SAVE_DIR, f\"000_home_{ts}.html\")\n",
        "        meta_block = (\n",
        "            \"<!--\\nMETA:\\n\"\n",
        "            f\"source_url: {BASE_URL}\\n\"\n",
        "            f\"title: Forums Home\\n\"\n",
        "            f\"scraped_at: {now_iso_bkk()}\\n\"\n",
        "            \"-->\\n\"\n",
        "        )\n",
        "        with open(home_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(meta_block + home_html)\n",
        "        print(f\"Saved HOME -> {home_path}\")\n",
        "\n",
        "        soup = BeautifulSoup(home_html, \"html.parser\")\n",
        "        anchors = soup.find_all(\"a\")\n",
        "\n",
        "        candidates = []\n",
        "        seen_urls = set()\n",
        "        for a in anchors:\n",
        "            text = (a.get_text(strip=True) or \"\")\n",
        "            href = a.get(\"href\")\n",
        "            if not href:\n",
        "                continue\n",
        "            url = abs_url(BASE_URL, href)\n",
        "            if not url:\n",
        "                continue\n",
        "            if tldextract.extract(url).registered_domain != domain_ok:\n",
        "                continue\n",
        "            if (\"/thread\" in url) or (\"/topic\" in url) or (\"/forums/\" in url):\n",
        "                if url not in seen_urls and text:\n",
        "                    seen_urls.add(url)\n",
        "                    candidates.append((text, url))\n",
        "\n",
        "        # จำกัดจำนวนกระทู้\n",
        "        thread_list = candidates[:MAX_THREADS]\n",
        "        if not thread_list:\n",
        "            print(\" ไม่พบลิงก์กระทู้จากหน้าแรก \")\n",
        "\n",
        "        idx = 0\n",
        "        for title_text, thread_url in thread_list:\n",
        "            idx += 1\n",
        "            try:\n",
        "                await page.goto(thread_url, wait_until=WAIT_UNTIL)\n",
        "                thread_html = await page.content()\n",
        "\n",
        "                # ตั้งชื่อไฟล์กระทู้\n",
        "                slug = slugify(title_text) or f\"thread-{idx:04d}\"\n",
        "                fname = f\"thread_{idx:04d}_{slug}.html\"\n",
        "                fpath = os.path.join(SAVE_DIR, fname)\n",
        "\n",
        "                # แทรก META block ไว้หัวไฟล์\n",
        "                meta_block = (\n",
        "                    \"<!--\\nMETA:\\n\"\n",
        "                    f\"source_url: {thread_url}\\n\"\n",
        "                    f\"title: {title_text}\\n\"\n",
        "                    f\"scraped_at: {now_iso_bkk()}\\n\"\n",
        "                    \"-->\\n\"\n",
        "                )\n",
        "                with open(fpath, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(meta_block + thread_html)\n",
        "\n",
        "                print(f\" Saved THREAD {idx:02d} -> {fpath}\")\n",
        "            except Exception as e:\n",
        "                print(f\" Error on thread {idx} ({thread_url}): {e}\")\n",
        "\n",
        "        await browser.close()\n",
        "\n",
        "await scrape_forum_html()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAatiESaI958",
        "outputId": "9125a769-a9b5-40e7-c80d-816d1b56a1f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1776887707.py:2: DeprecationWarning: The 'registered_domain' property is deprecated and will be removed in the next major version. Use 'top_domain_under_public_suffix' instead, which has the same behavior but a more accurate name.\n",
            "  domain_ok = tldextract.extract(BASE_URL).registered_domain\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved HOME -> /content/drive/MyDrive/agnos-rag/data/raw/000_home_20250917-144838.html\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1776887707.py:40: DeprecationWarning: The 'registered_domain' property is deprecated and will be removed in the next major version. Use 'top_domain_under_public_suffix' instead, which has the same behavior but a more accurate name.\n",
            "  if tldextract.extract(url).registered_domain != domain_ok:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Saved THREAD 01 -> /content/drive/MyDrive/agnos-rag/data/raw/thread_0001_20-Acute-pericarditis72520222-15-8.html\n",
            " Saved THREAD 02 -> /content/drive/MyDrive/agnos-rag/data/raw/thread_0002_24-Dermatitis-unspecified7222022-5.html\n",
            " Saved THREAD 03 -> /content/drive/MyDrive/agnos-rag/data/raw/thread_0003_23-Major-depressive-disorder21820243-2-3-5.html\n",
            " Saved THREAD 04 -> /content/drive/MyDrive/agnos-rag/data/raw/thread_0004_20-Menieres-disease82220222-4-5-2.html\n",
            " Saved THREAD 05 -> /content/drive/MyDrive/agnos-rag/data/raw/thread_0005_21-Cystitis21620241-2.html\n",
            " Saved THREAD 06 -> /content/drive/MyDrive/agnos-rag/data/raw/thread_0006_untitled.html\n",
            " Saved THREAD 07 -> /content/drive/MyDrive/agnos-rag/data/raw/thread_0007_20-Cellulitis21520242-3-4-0.html\n",
            " Saved THREAD 08 -> /content/drive/MyDrive/agnos-rag/data/raw/thread_0008_20-Dysmenorrhea-unpecified112720233-20-27-3-2-6-0.html\n",
            " Saved THREAD 09 -> /content/drive/MyDrive/agnos-rag/data/raw/thread_0009_17-Chancroid11262023-555-555-0.html\n",
            " Saved THREAD 10 -> /content/drive/MyDrive/agnos-rag/data/raw/thread_0010_untitled.html\n"
          ]
        }
      ]
    }
  ]
}